{"config":{"lang":["pt"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Solu\u00e7\u00e3o definitiva para Big Data em ambientes com Hardware Limitado  \ud83d\ude80 Alta Performance <p>       Esque\u00e7a o erro <code>Out of Memory</code>. Nossa arquitetura l\u00ea arquivos maiores que a RAM dispon\u00edvel, utilizando buffers inteligentes e I/O n\u00e3o bloqueante. Processa 10GB com apenas 512MB de RAM.     </p> \ud83d\udee1\ufe0f SLA Autom\u00e1tico <p>       O sistema classifica automaticamente a qualidade das colunas (Ouro, Prata, Bronze) calculando a densidade de informa\u00e7\u00e3o e consist\u00eancia em tempo real.     </p> \ud83d\udc41\ufe0f Seguran\u00e7a &amp; LGPD <p>       Detector de PII (Dados Pessoais) integrado. O sistema varre e alerta sobre CPF, E-mails e Cart\u00f5es de Cr\u00e9dito expostos.     </p> \ud83d\udce6 Single Binary <p>       Zero depend\u00eancias. O Backend (Go) e o Frontend (React) s\u00e3o compilados em um \u00fanico arquivo execut\u00e1vel <code>.exe</code>.     </p> \ud83e\udde0 Infer\u00eancia Inteligente <p>       Esque\u00e7a o mapeamento manual. O algoritmo de Type Inference analisa amostras dos dados para detectar tipos automaticamente.     </p> \ud83d\udcca Interface &amp; Estat\u00edsticas <p>       Frontend em React + Material UI. DataGrid com pagina\u00e7\u00e3o nativa e estat\u00edsticas (M\u00e9dia, Desvio Padr\u00e3o) em tempo real.     </p> <p></p> <p></p> A Engenharia por tr\u00e1s do Streaming <p>   O diferencial do DataProfiler \u00e9 a arquitetura Producer-Consumer.   O dado flui atrav\u00e9s de canais concorrentes sem nunca ser carregado totalmente na mem\u00f3ria. </p> <pre><code>\ngraph LR\n    %% --- Defini\u00e7\u00e3o dos N\u00f3s ---\n    A[Arquivo CSV Massivo] --&gt;|Stream Leitura| B(Go Reader / Buffer);\n    B --&gt;|Chunks de Dados| C{Canal de Distribui\u00e7\u00e3o};\n\n    %% Workers paralelos\n    C --&gt;|Worker 1| D[Valida\u00e7\u00e3o de Tipos];\n    C --&gt;|Worker 2| E[Regex PII];\n    C --&gt;|Worker 3| F[Estat\u00edstica];\n\n    %% Agrega\u00e7\u00e3o\n    D &amp; E &amp; F --&gt;|Agrega\u00e7\u00e3o| G[Relat\u00f3rio JSON];\n    G --&gt; H[Dashboard React];\n\n    %% --- APLICA\u00c7\u00c3O DE CLASSES CSS EXTERNAS ---\n    %% Isso vincula os n\u00f3s \u00e0s regras que criamos no home.css\n    %% N\u00e3o definimos cores aqui. O CSS controla tudo.\n\n    class A,B source;\n    class C,D,E,F,G process;\n    class H target;\n\n    %% Apenas removemos o preenchimento padr\u00e3o da linha para o CSS pintar\n    linkStyle default fill:none;\n</code></pre> Figura 1: Fluxo de Dados na Arquitetura Producer-Consumer Jornada de Evolu\u00e7\u00e3o \u2699\ufe0f Fase 1: O Motor Matem\u00e1tico <ul> <li>Core estat\u00edstico de alta precis\u00e3o (Go)</li> <li>Infer\u00eancia de Tipos com Regex Engine</li> <li>Arquitetura In-Memory (MVP)</li> </ul> \u2713 \u2713 \ud83c\udf0a Fase 2: Streaming &amp; Robustez <ul> <li>Pipeline de Leitura (Channels)</li> <li>Gest\u00e3o de Mem\u00f3ria (Sync.Pool)</li> <li>Observabilidade (Slog &amp; Pprof)</li> </ul> \ud83c\udfa8 Fase 3: Experi\u00eancia Enterprise <ul> <li>Interface Material UI (DataGrid)</li> <li>Feedback Visual (SSE Real-time)</li> <li>Empacotamento Docker &amp; Embed Binary</li> </ul> \u2713 \ud83d\udd2e O Futuro (Roadmap) <ul> <li>Persist\u00eancia (SQLite/Postgres)</li> <li>Cardinalidade (HyperLogLog)</li> <li>Exporta\u00e7\u00e3o de Relat\u00f3rios PDF</li> </ul> Pronto para usar?  Baixar para Windows (.exe)   Ver Decis\u00f5es de Arquitetura (ADR)  \ud83d\udc77 Junte-se ao Desenvolvimento <p> Este projeto segue padr\u00f5es rigorosos de engenharia. Quer contribuir com c\u00f3digo ou documenta\u00e7\u00e3o? Confira nosso Guia de Estilo e Padr\u00f5es de Commit. </p>  Ler Guia de Contribui\u00e7\u00e3o   Ver no GitHub"},{"location":"CONTRIBUTING/","title":"Guia de Contribui\u00e7\u00e3o e Estilo","text":""},{"location":"CONTRIBUTING/#guia-de-contribuicao-e-estilo","title":"Guia de Contribui\u00e7\u00e3o e Estilo","text":"<p>Este documento define os padr\u00f5es t\u00e9cnicos, visuais e de reda\u00e7\u00e3o para a documenta\u00e7\u00e3o do Data Profiler.</p> <p>Como utilizamos o Material for MkDocs com extens\u00f5es avan\u00e7adas, \u00e9 obrigat\u00f3rio seguir estas diretrizes para manter a consist\u00eancia e o n\u00edvel \"Enterprise Grade\" do projeto.</p>"},{"location":"CONTRIBUTING/#1-principios-de-redacao","title":"1. Princ\u00edpios de Reda\u00e7\u00e3o","text":""},{"location":"CONTRIBUTING/#11-tom-de-voz","title":"1.1. Tom de Voz","text":"<p>A documenta\u00e7\u00e3o deve ser t\u00e9cnica, impessoal e direta. Evite narrativas em primeira pessoa.</p> <ul> <li>\u274c Incorreto (Pessoal/Coloquial): \"A\u00ed o c\u00f3digo pega o arquivo...\" ou \"Eu decidi usar Go porque...\"</li> <li>\u2705 Correto (T\u00e9cnico/Passivo): \"O sistema processa o arquivo visando otimiza\u00e7\u00e3o de I/O.\" ou \"A decis\u00e3o arquitetural baseou-se na lat\u00eancia.\"</li> </ul>"},{"location":"CONTRIBUTING/#12-emojis-e-icones","title":"1.2. Emojis e \u00cdcones","text":"<ul> <li>Texto Corrido: Proibido usar emojis coloridos (\ud83d\ude0e, \ud83d\ude80, \ud83d\udd25) no meio de frases ou t\u00edtulos. Isso reduz a seriedade da documenta\u00e7\u00e3o.</li> <li>Admonitions e Bot\u00f5es: Permitido o uso de \u00edcones monocrom\u00e1ticos via sintaxe <code>:icon-name:</code>.</li> <li>Exemplo: Use <code>:material-github:</code> para links de reposit\u00f3rio.</li> </ul>"},{"location":"CONTRIBUTING/#13-formatacao-de-texto","title":"1.3. Formata\u00e7\u00e3o de Texto","text":"<p>Utilize as extens\u00f5es visuais para destacar elementos de interface ou termos chave.</p> <ul> <li> <p>Teclas de Atalho (<code>pymdownx.keys</code>): Use para documentar atalhos da CLI ou do Dashboard.</p> </li> <li> <p>Sintaxe: <code>++ctrl+c++</code> ou <code>++enter++</code></p> </li> <li> <p>Resultado: Renderiza uma tecla visual: Ctrl+C</p> </li> <li> <p>Marca\u00e7\u00e3o de Texto (<code>pymdownx.mark</code>): Use para destacar um termo cr\u00edtico em uma frase (diferente do negrito).</p> </li> <li>Sintaxe: <code>==termo cr\u00edtico==</code></li> <li>Resultado: Fundo amarelo estilo \"marca-texto\": termo cr\u00edtico</li> </ul>"},{"location":"CONTRIBUTING/#2-padrao-para-codigo-go-react","title":"2. Padr\u00e3o para C\u00f3digo (Go &amp; React)","text":"<p>Nunca copie e cole c\u00f3digo fonte manualmente nos arquivos Markdown. Isso gera documenta\u00e7\u00e3o obsoleta.</p>"},{"location":"CONTRIBUTING/#21-importacao-dinamica-snippets","title":"2.1. Importa\u00e7\u00e3o Din\u00e2mica (Snippets)","text":"<p>Feature: <code>pymdownx.snippets</code></p> <p>Garanta que a documenta\u00e7\u00e3o seja a \u00danica Fonte da Verdade. Em vez de copiar e colar, fa\u00e7a o MkDocs ler o arquivo real no momento do build usando o <code>--8&lt;--</code>. Sempre envolva a importa\u00e7\u00e3o no bloco da linguagem correta (ex: <code>```go</code>) para manter o syntax highlighting.</p> <p>Como usar:</p> <p>I. Arquivo Inteiro:</p> internal/profiler/pool.go<pre><code>package profiler\n\nimport \"sync\"\n\nvar RowPool = sync.Pool{\n    New: func() any {\n        return make([]string, 0, 10)\n    },\n}\n\nfunc GetRowSlice() []string {\n    return RowPool.Get().([]string)\n}\n\nfunc PutRowSlice(row []string) {\n    RowPool.Put(row[:0])\n}\n</code></pre> <p>II. Apenas Trechos (Linhas Espec\u00edficas): Use <code>:inicio:fim</code> para focar apenas no que importa.</p> <pre><code>func main() {\n\n    cliMode := flag.Bool(\"cli\", false, \"Rodar em modo CLI (terminal) sem servidor web\")\n    filePath := flag.String(\"file\", \"\", \"Caminho do arquivo CSV para processar (obrigat\u00f3rio no modo -cli)\")\n\n    flag.Parse()\n\n    var logOutput *os.File\n    if *cliMode {\n        logOutput = os.Stderr\n    } else {\n        logOutput = os.Stdout\n    }\n\n    logger := slog.New(slog.NewJSONHandler(logOutput, nil))\n\n    slog.SetDefault(logger)\n\n    if *cliMode {\n        if *filePath == \"\" {\n            slog.Error(\"Erro: No modo -cli, forne\u00e7a o arquivo: -file=\\\"dados.csv\\\"\")\n            os.Exit(1)\n        }\n        runCLI(logger, *filePath)\n        return\n    }\n\n    runServer()\n\n}\n</code></pre> <p>III. Por Se\u00e7\u00e3o Nomeada (Recomendado): Se adicionar marcadores no c\u00f3digo Go (<code>// --  8&lt; -- [start:nome] c\u00f3digo -- 8&lt; -- [end:nome]</code>), use:</p> <pre><code>var (\n    // Essenciais\n    RegexFiscalKey = regexp.MustCompile(`^\\d{44}$`)                          // NFe, CTe, MDFe\n    RegexPlaca     = regexp.MustCompile(`^[A-Z]{3}-?[0-9][0-9A-Z][0-9]{2}$`) // ABC1234 ou ABC1C34\n\n    // Documentos Brasileiros\n    RegexCPF  = regexp.MustCompile(`^\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}$`)       // 000.000.000-00\n    RegexCNPJ = regexp.MustCompile(`^\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}$`) // 00.000.000/0000-00\n\n    // Datas (Formatos comuns BR e ISO)\n    RegexDateBr  = regexp.MustCompile(`^\\d{2}/\\d{2}/\\d{4}$`) // DD/MM/YYYY\n    RegexDateIso = regexp.MustCompile(`^\\d{4}-\\d{2}-\\d{2}$`) // YYYY-MM-DD\n\n    // Log\u00edstica Avan\u00e7ada\n    RegexContainer = regexp.MustCompile(`^[A-Z]{4}\\d{7}$`)                  // Padr\u00e3o ISO\n    Regex8Digits   = regexp.MustCompile(`^\\d{8}$`)                          // NCM, RNTRC, CEP sem tra\u00e7o, Data compacta\n    Regex11Digits  = regexp.MustCompile(`^\\d{11}$`)                         // CPF sem formata\u00e7\u00e3o\n    RegexCEP       = regexp.MustCompile(`^\\d{5}-\\d{3}$`)                    // CEP com tra\u00e7o\n    RegexMobile    = regexp.MustCompile(`^\\(?\\d{2}\\)?\\s?9\\d{4}-?\\d{4}$`)    // Celular com 9\n    RegexEmail     = regexp.MustCompile(`^[\\w-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}$`) // Email\n    RegexEAN       = regexp.MustCompile(`^\\d{13,14}$`)                      // GTIN/EAN (Produtos)\n)\n</code></pre>"},{"location":"CONTRIBUTING/#22-blocos-explicativos-e-anotacoes","title":"2.2. Blocos Explicativos e Anota\u00e7\u00f5es","text":"<p>Feature: <code>pymdownx.highlight</code> Para trechos te\u00f3ricos, use a sintaxe de linguagem e anota\u00e7\u00f5es num\u00e9ricas para explicar a l\u00f3gica sem poluir o c\u00f3digo com coment\u00e1rios excessivos.</p> <p>Exemplo de uso:</p> internal/pool.go<pre><code>func (p *Pool) Start() {\n    for i := 0; i &lt; p.workers; i++ {\n        go p.worker(p.jobs, p.results) // (1)\n    }\n}\n</code></pre> <ol> <li>Inicializa as goroutines baseadas na flag <code>-cpu</code>.</li> </ol>"},{"location":"CONTRIBUTING/#3-componentes-visuais-layout","title":"3. Componentes Visuais (Layout)","text":"<p>Utilize os componentes abaixo para organizar a informa\u00e7\u00e3o e evitar \"paredes de texto\".</p>"},{"location":"CONTRIBUTING/#31-abas-tabs","title":"3.1. Abas (Tabs)","text":"<p>Feature: <code>pymdownx.tabbed</code> Quando usar: Obrigat\u00f3rio para comparar instru\u00e7\u00f5es (Ex: Docker vs Local) ou linguagens (JSON vs YAML).</p> <p>Sintaxe:</p> DockerLocal (Go) <pre><code>    docker-compose up\n</code></pre> <pre><code>    go run main.go\n</code></pre> <p>!!! warning \"Regra de Ouro das Abas\" Para manter a leitura do c\u00f3digo-fonte limpa, adicione sempre uma linha em branco ap\u00f3s o t\u00edtulo da aba.</p> <pre><code>O conte\u00fado interno deve obrigatoriamente ter **4 espa\u00e7os de recuo** (aperte `Tab` duas vezes) em rela\u00e7\u00e3o \u00e0 margem esquerda.\n\n**Exemplo Correto:**\n\n````markdown\n=== \"Aba Exemplo\"\n&lt;ENTER&gt;\n&lt;TAB&gt;&lt;TAB&gt; `bash\n&lt;TAB&gt;&lt;TAB&gt; comando aqui\n&lt;TAB&gt;&lt;TAB&gt; `\n````\n</code></pre>"},{"location":"CONTRIBUTING/#32-detalhes-collapsible","title":"3.2. Detalhes (Collapsible)","text":"<p>Feature: <code>pymdownx.details</code> Quando usar: Obrigat\u00f3rio para JSONs grandes, Logs de erro ou configura\u00e7\u00f5es extensas que n\u00e3o precisam ser lidas imediatamente.</p> <p>Sintaxe:</p> <pre><code>??? info \"Ver Payload JSON completo\"\n`json {json: fechado e escondido direto}`\n???+ info \"Ver Payload JSON completo\"\n`json {json: aberto e vis\u00edvel direto}`\n</code></pre> Ver Payload JSON completo <pre><code>{\"time\":\"2025-12-31T03:20:44.2987599-03:00\",\"level\":\"INFO\",\"msg\":\"\u00ad\u0192\u00f6\u00ba Servidor Debug/Pprof iniciado\",\"addr\":\"localhost:6060\"}\n{\"time\":\"2025-12-31T03:20:44.2987599-03:00\",\"level\":\"INFO\",\"msg\":\"Iniciando servidor DataProfiler\",\"port\":8080,\"env\":\"production\",\"version\":\"v1.0.0\"}\n{\"time\":\"2025-12-31T03:20:44.3207746-03:00\",\"level\":\"INFO\",\"msg\":\"Servidor pronto e escutando\",\"addr\":\":8080\"}\n{\"time\":\"2025-12-31T03:20:53.645604-03:00\",\"level\":\"INFO\",\"msg\":\"Nova requisi\u251c\u00ba\u251c\u00fao de upload recebida\",\"req_id\":1767162053645604000,\"method\":\"POST\",\"path\":\"/api/upload\"}\n</code></pre>"},{"location":"CONTRIBUTING/#33-admonitions-alertas","title":"3.3. Admonitions (Alertas)","text":"<p>Use para destacar informa\u00e7\u00f5es cr\u00edticas. Escolha o tipo correto para o contexto:</p> Tipo Sintaxe Cor Contexto de Uso Nota <code>!!! note</code> \ud83d\udd35 Azul Observa\u00e7\u00f5es gerais. Dica <code>!!! tip</code> \ud83d\udfe2 Verde Melhores pr\u00e1ticas e atalhos. Sucesso <code>!!! success</code> \ud83d\udfe2 Verde Resultado esperado ou confirma\u00e7\u00e3o de sucesso. Aviso <code>!!! warning</code> \ud83d\udfe0 Laranja Valida\u00e7\u00f5es de dados e aten\u00e7\u00e3o. Perigo <code>!!! danger</code> \ud83d\udd34 Vermelho Risco de crash, perda de dados ou PII. Bug <code>!!! bug</code> \ud83d\udd34 Vermelho Erros conhecidos ou limita\u00e7\u00f5es da vers\u00e3o. Exemplo <code>!!! example</code> \ud83d\udfe3 Roxo Casos de uso e amostras de c\u00f3digo."},{"location":"CONTRIBUTING/#4-diagramas-e-listas","title":"4. Diagramas e Listas","text":""},{"location":"CONTRIBUTING/#41-diagramas-de-arquitetura","title":"4.1. Diagramas de Arquitetura","text":"<p>Feature: <code>mermaid</code> Regra: Proibido usar imagens est\u00e1ticas (<code>.png</code>, <code>.jpg</code>) para fluxos. Elas ficam desatualizadas e s\u00e3o dif\u00edceis de editar. Use Mermaid.js.</p> <p>Sintaxe:</p> <pre><code>graph LR\n    A[Upload CSV] --&gt;|Stream| B(Go Backend)\n    B --&gt;|Process| C{Valida\u00e7\u00e3o}\n    C -- Erro --&gt; D[Log]\n    C -- Ok --&gt; E[Stats]\n</code></pre>"},{"location":"CONTRIBUTING/#42-listas-de-tarefas","title":"4.2. Listas de Tarefas","text":"<p>Feature: <code>pymdownx.tasklist</code> Use para Roadmaps ou Checklists de Deploy.</p> <ul> <li> Milestone 1 (Conclu\u00eddo)</li> <li> Milestone 2 (Pendente)</li> </ul>"},{"location":"CONTRIBUTING/#5-matematica-e-estatistica","title":"5. Matem\u00e1tica e Estat\u00edstica","text":"<p>Como o projeto possui um core estat\u00edstico (<code>StatsCalc</code>), utilize LaTeX para documentar f\u00f3rmulas. Nunca use prints de f\u00f3rmulas.</p> <ul> <li>Inline: Use <code>$</code> para citar vari\u00e1veis como ou .</li> <li>Bloco: Use <code>$$</code> para equa\u00e7\u00f5es completas.   $\\(\\sigma = \\sqrt{\\frac{\\sum(x - \\mu)^2}{N}}\\)$</li> </ul>"},{"location":"CONTRIBUTING/#6-organizacao-de-arquivos","title":"6. Organiza\u00e7\u00e3o de Arquivos","text":"<p>Utilizamos o plugin <code>awesome-pages</code> para navega\u00e7\u00e3o descentralizada.</p> <ol> <li>Nomes de Arquivo: Devem ser sempre em <code>kebab-case</code> (ex: <code>arquitetura-streaming.md</code> e n\u00e3o <code>ArquiteturaStreaming.md</code>).</li> <li>Meta Arquivos: Todo diret\u00f3rio novo deve conter um arquivo <code>.pages.yml</code> configurando a ordem de exibi\u00e7\u00e3o dos itens.</li> </ol>"},{"location":"engineering/","title":"Vis\u00e3o Geral da Engenharia","text":""},{"location":"engineering/#visao-geral-da-engenharia","title":"Vis\u00e3o Geral da Engenharia","text":"<p>O Data Profiler \u00e9 projetado sob uma arquitetura h\u00edbrida de alto desempenho, combinando a robustez de sistemas de baixo n\u00edvel (Go) com a interatividade de interfaces modernas (React).</p> <p>O objetivo principal da engenharia deste projeto \u00e9 viabilizar o Processamento Out-of-Core: a capacidade de analisar datasets significativamente maiores que a mem\u00f3ria RAM dispon\u00edvel na m\u00e1quina hospedeira.</p>"},{"location":"engineering/#1-diagrama-de-contexto-c4-nivel-1","title":"1. Diagrama de Contexto (C4 N\u00edvel 1)","text":"<p>O sistema opera como um execut\u00e1vel monol\u00edtico autocontido, eliminando depend\u00eancias externas (como Runtime Node.js ou Banco de Dados) para a execu\u00e7\u00e3o padr\u00e3o.</p> <pre><code>graph TD\n    User((Usu\u00e1rio))\n\n    subgraph \"Data Profiler (Single Binary)\"\n        UI[Frontend SPA&lt;br/&gt;React + Vite]\n        API[Backend API&lt;br/&gt;Go net/http]\n        Engine[Profiler Engine&lt;br/&gt;Go Routines]\n    end\n\n    FileSystem[(Sistema de Arquivos)]\n\n    User &lt;--&gt;|HTTP/SSE| UI\n    UI &lt;--&gt;|JSON/Stream| API\n    API &lt;--&gt;|Stream I/O| FileSystem\n    API &lt;--&gt;|Channels| Engine\n</code></pre> Figura 1: Fluxo do funcionalmente"},{"location":"engineering/#2-principios-de-design","title":"2. Princ\u00edpios de Design","text":""},{"location":"engineering/#21-streaming-first-zero-load","title":"2.1. Streaming First (Zero Load)","text":"<p>Diferente de abordagens tradicionais que carregam o arquivo inteiro (<code>ReadAll</code>), todo o pipeline do Data Profiler \u00e9 baseado em Streaming.</p> <ul> <li>Leitura: O arquivo \u00e9 lido em chunks de 4KB a 32KB.</li> <li>Processamento: As linhas fluem por canais (<code>chan []string</code>) para workers.</li> <li>Resultado: As estat\u00edsticas s\u00e3o acumuladas incrementalmente, mantendo o consumo de mem\u00f3ria O(1).</li> </ul>"},{"location":"engineering/#22-feedback-loop-em-tempo-real","title":"2.2. Feedback Loop em Tempo Real","text":"<p>Processamentos de Big Data podem demorar. Para evitar a sensa\u00e7\u00e3o de travamento:</p> <ul> <li>O Backend n\u00e3o bloqueia a requisi\u00e7\u00e3o HTTP de upload.</li> <li>O Frontend utiliza Server-Sent Events (SSE) para receber atualiza\u00e7\u00f5es parciais (0% a 100%).</li> <li>O usu\u00e1rio visualiza o progresso linha a linha, n\u00e3o apenas o resultado final.</li> </ul>"},{"location":"engineering/#23-distribuicao-simplificada","title":"2.3. Distribui\u00e7\u00e3o Simplificada","text":"<p>A complexidade da stack (Node.js, Go, Bibliotecas) \u00e9 abstra\u00edda no processo de build.</p> <ul> <li>O Frontend \u00e9 compilado estaticamente (<code>npm run build</code>).</li> <li>Os assets s\u00e3o embutidos no bin\u00e1rio Go (<code>//go:embed</code>).</li> <li>O resultado \u00e9 um \u00fanico arquivo <code>.exe</code> ou bin\u00e1rio Linux que sobe todo o ambiente.</li> </ul>"},{"location":"engineering/#3-navegacao-da-documentacao-tecnica","title":"3. Navega\u00e7\u00e3o da Documenta\u00e7\u00e3o T\u00e9cnica","text":"<p>Para aprofundar-se em cada camada da aplica\u00e7\u00e3o, consulte as se\u00e7\u00f5es espec\u00edficas:</p> <p>Entenda a concorr\u00eancia, o Worker Pool, a infer\u00eancia de tipos com Regex e a matem\u00e1tica estat\u00edstica. Explorar Backend</p> <p>Veja a arquitetura de componentes, o consumo de eventos SSE e o Design System com Material UI. Explorar Frontend</p> <p>Hist\u00f3rico das decis\u00f5es t\u00e9cnicas dif\u00edceis, trade-offs e escolhas de bibliotecas. Ver Decis\u00f5es</p>"},{"location":"engineering/#backend-go","title":"Backend (Go)","text":""},{"location":"engineering/#frontend-react","title":"Frontend (React)","text":""},{"location":"engineering/#decisoes-de-arquitetura-adr","title":"Decis\u00f5es de Arquitetura (ADR)","text":""},{"location":"engineering/#4-stack-tecnologica","title":"4. Stack Tecnol\u00f3gica","text":"Camada Tecnologia Vers\u00e3o Prop\u00f3sito Linguagem Core Go (Golang) <code>1.23+</code> Performance, Concorr\u00eancia e Type Safety. Interface React <code>19.0</code> Renderiza\u00e7\u00e3o de UI e Gerenciamento de Estado. Build Tool Vite <code>7.0</code> Bundling r\u00e1pido e HMR para desenvolvimento. Transporte net/http Stdlib Servidor Web e implementa\u00e7\u00e3o de SSE. Container Docker <code>Compose</code> Padroniza\u00e7\u00e3o de ambiente de desenvolvimento."},{"location":"engineering/backend/","title":"Engenharia Backend (Go)","text":""},{"location":"engineering/backend/#engenharia-backend-go","title":"Engenharia Backend (Go)","text":"<p>O backend do Data Profiler \u00e9 um motor de alta performance escrito em Go (Golang), projetado para processar arquivos maiores que a mem\u00f3ria RAM dispon\u00edvel (Out-of-Core Processing).</p> <p>A arquitetura segue o padr\u00e3o Producer-Consumer utilizando primitivas de concorr\u00eancia nativas da linguagem (Goroutines e Channels).</p>"},{"location":"engineering/backend/#1-arquitetura-de-concorrencia","title":"1. Arquitetura de Concorr\u00eancia","text":"<p>Diferente de abordagens tradicionais que carregam o arquivo inteiro na mem\u00f3ria (<code>ReadAll</code>), nosso sistema cria um pipeline de processamento cont\u00ednuo.</p> <pre><code>graph LR\n    Input[CSV File] --&gt;|Stream| Loader(CSV Loader)\n    Loader --&gt;|Jobs| Channel{Job Channel}\n\n    subgraph \"Worker Pool (Paralelo)\"\n        Channel --&gt; W1[Worker 1]\n        Channel --&gt; W2[Worker 2]\n        Channel --&gt; W3[Worker 3]\n    end\n\n    W1 --&gt;|Stats Parciais| Accumulator[Accumulator]\n    W2 --&gt;|Stats Parciais| Accumulator\n    W3 --&gt;|Stats Parciais| Accumulator\n\n    Accumulator --&gt;|JSON Final| Output[Relat\u00f3rio]\n</code></pre> Figura 1: Fluxo de dentro do Backend"},{"location":"engineering/backend/#componentes-principais","title":"Componentes Principais","text":"Componente Responsabilidade Infra (Loader) L\u00ea o arquivo linha a linha e envia para o canal. Atua como Producer. Profiler (Pool) Gerencia um conjunto de Workers que processam dados em paralelo. Accumulator Centraliza os resultados parciais, evitando Race Conditions cr\u00edticas."},{"location":"engineering/backend/#2-worker-pool-e-paralelismo","title":"2. Worker Pool e Paralelismo","text":"<p>O cora\u00e7\u00e3o do sistema \u00e9 o <code>Worker Pool</code>. Ele limita o n\u00famero de Goroutines ativas para evitar Context Switching excessivo e exaust\u00e3o de CPU.</p> <p>O n\u00famero de workers \u00e9 definido dinamicamente baseado nas CPUs dispon\u00edveis ou via configura\u00e7\u00e3o.</p> internal/profiler/pool.go<pre><code>package profiler\n\nimport \"sync\"\n\nvar RowPool = sync.Pool{\n    New: func() any {\n        return make([]string, 0, 10)\n    },\n}\n\nfunc GetRowSlice() []string {\n    return RowPool.Get().([]string)\n}\n\nfunc PutRowSlice(row []string) {\n    RowPool.Put(row[:0])\n}\n</code></pre> <p>Cpu Bound vs I/O Bound</p> <p>Como o gargalo do sistema \u00e9 o processamento de texto (Regex/Parsing) e n\u00e3o a leitura de disco, a estrat\u00e9gia de m\u00faltiplos workers escala linearmente com o n\u00famero de n\u00facleos do processador.</p>"},{"location":"engineering/backend/#3-inferencia-de-tipos-core-logic","title":"3. Infer\u00eancia de Tipos (Core Logic)","text":"<p>A detec\u00e7\u00e3o de tipos \u00e9 feita atrav\u00e9s de express\u00f5es regulares (Regex) otimizadas e compiladas na inicializa\u00e7\u00e3o do pacote. O sistema tenta inferir o tipo mais espec\u00edfico poss\u00edvel (Int &gt; Float &gt; String).</p> <p>A l\u00f3gica \u00e9 isolada em fun\u00e7\u00f5es puras para facilitar Testes Unit\u00e1rios e Fuzzing.</p> internal/profiler/infer.go<pre><code>package profiler\n\nimport (\n    \"regexp\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n)\n\nvar (\n    // Essenciais\n    RegexFiscalKey = regexp.MustCompile(`^\\d{44}$`)                          // NFe, CTe, MDFe\n    RegexPlaca     = regexp.MustCompile(`^[A-Z]{3}-?[0-9][0-9A-Z][0-9]{2}$`) // ABC1234 ou ABC1C34\n\n    // Documentos Brasileiros\n    RegexCPF  = regexp.MustCompile(`^\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}$`)       // 000.000.000-00\n    RegexCNPJ = regexp.MustCompile(`^\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}$`) // 00.000.000/0000-00\n\n    // Datas (Formatos comuns BR e ISO)\n    RegexDateBr  = regexp.MustCompile(`^\\d{2}/\\d{2}/\\d{4}$`) // DD/MM/YYYY\n    RegexDateIso = regexp.MustCompile(`^\\d{4}-\\d{2}-\\d{2}$`) // YYYY-MM-DD\n\n    // Log\u00edstica Avan\u00e7ada\n    RegexContainer = regexp.MustCompile(`^[A-Z]{4}\\d{7}$`)                  // Padr\u00e3o ISO\n    Regex8Digits   = regexp.MustCompile(`^\\d{8}$`)                          // NCM, RNTRC, CEP sem tra\u00e7o, Data compacta\n    Regex11Digits  = regexp.MustCompile(`^\\d{11}$`)                         // CPF sem formata\u00e7\u00e3o\n    RegexCEP       = regexp.MustCompile(`^\\d{5}-\\d{3}$`)                    // CEP com tra\u00e7o\n    RegexMobile    = regexp.MustCompile(`^\\(?\\d{2}\\)?\\s?9\\d{4}-?\\d{4}$`)    // Celular com 9\n    RegexEmail     = regexp.MustCompile(`^[\\w-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}$`) // Email\n    RegexEAN       = regexp.MustCompile(`^\\d{13,14}$`)                      // GTIN/EAN (Produtos)\n)\n\n\nfunc InferType(value string, headerName string) DataType {\n    if value == \"\" {\n        return TypeEmpty\n    }\n    headerLower := strings.ToLower(headerName)\n\n    if Regex8Digits.MatchString(value) {\n        if containsAny(headerLower, \"ncm\", \"fiscal\", \"classificacao\", \"sh\") {\n            return TypeNCM\n        }\n        if containsAny(headerLower, \"rntrc\", \"antt\", \"transportador\") {\n            return TypeRNTRC\n        }\n        if containsAny(headerLower, \"cep\", \"zip\", \"postal\") {\n            return TypeCEP\n        }\n        if isCompactDate(value) {\n            return TypeDateCompact\n        }\n    }\n\n    if RegexFiscalKey.MatchString(value) {\n        return TypeFiscalKey44\n    }\n    if RegexEmail.MatchString(value) {\n        return TypeEmail\n    }\n    if RegexPlaca.MatchString(value) {\n        return TypePlaca\n    }\n    if RegexContainer.MatchString(value) {\n        return TypeContainer\n    }\n    if RegexCEP.MatchString(value) {\n        return TypeCEP\n    }\n    if RegexMobile.MatchString(value) {\n        return TypeMobile\n    }\n\n    if RegexDateBr.MatchString(value) || RegexDateIso.MatchString(value) {\n        return TypeDate\n    }\n\n    if RegexEAN.MatchString(value) {\n        if containsAny(headerLower, \"ean\", \"gtin\", \"barras\", \"item\", \"produto\", \"sku\") {\n            return TypeEAN\n        }\n\n        if containsAny(headerLower, \"cnpj\", \"fornecedor\", \"empresa\", \"transportadora\") {\n            return TypeCNPJ\n        }\n    }\n\n    if RegexCNPJ.MatchString(value) {\n        return TypeCNPJ\n    }\n\n    if Regex11Digits.MatchString(value) {\n        if containsAny(headerLower, \"cpf\", \"cliente\", \"consumidor\", \"pessoa\", \"colaborador\", \"funcionario\", \"funcion\u00e1rio\", \"usuario\", \"usu\u00e1rio\", \"rg\", \"identidade\", \"documento\") {\n            return TypeCPF\n        }\n    }\n    if RegexCPF.MatchString(value) {\n        return TypeCPF\n    }\n\n    if isInt(value) {\n        return TypeInteger\n    }\n    if isFloat(value) {\n        return TypeFloat\n    }\n    if isBool(value) {\n        return TypeBoolean\n    }\n\n    return TypeString\n}\n\nfunc containsAny(text string, keywords ...string) bool {\n    for _, k := range keywords {\n        if strings.Contains(text, k) {\n            return true\n        }\n    }\n    return false\n}\n\nfunc isCompactDate(value string) bool {\n    _, err := time.Parse(\"20060102\", value)\n    if err == nil {\n        return true\n    }\n    _, err = time.Parse(\"02012006\", value)\n    return err == nil\n}\n\nfunc isInt(value string) bool {\n    _, err := strconv.Atoi(value)\n\n    return err == nil\n}\n\nfunc isBool(value string) bool {\n    lower := strings.ToLower(value)\n    return lower == \"true\" || lower == \"false\" || lower == \"s\" || lower == \"n\"\n}\n\nfunc isFloat(value string) bool {\n    value = strings.Replace(value, \",\", \".\", 1)\n    _, err := strconv.ParseFloat(value, 64)\n    return err == nil\n}\n</code></pre>"},{"location":"engineering/backend/#4-estrategia-de-streaming-memoria","title":"4. Estrat\u00e9gia de Streaming (Mem\u00f3ria)","text":"<p>Para garantir que o consumo de mem\u00f3ria permane\u00e7a constante (O(1)) independentemente do tamanho do arquivo, utilizamos <code>bufio.Scanner</code> e processamento em chunks.</p> <ol> <li>O arquivo \u00e9 aberto via <code>multipart.File</code>.</li> <li>Um <code>sniff</code> inicial l\u00ea os primeiros 4KB para detectar separadores e encoding.</li> <li>O cursor volta ao in\u00edcio e o streaming come\u00e7a.</li> </ol> internal/infra/csv_loader.go<pre><code>package infra\n\nimport (\n    \"bufio\"\n    \"context\"\n    \"encoding/csv\"\n    \"encoding/json\"\n    \"errors\"\n    \"fmt\"\n    \"io\"\n    \"log/slog\"\n    \"os\"\n    \"path/filepath\"\n    \"sort\"\n    \"unicode\"\n    \"unicode/utf8\"\n\n    \"github.com/JGustavoCN/dataprofiler/internal/profiler\"\n    \"golang.org/x/text/encoding/charmap\"\n    unicodeenc \"golang.org/x/text/encoding/unicode\"\n    \"golang.org/x/text/transform\"\n)\n\nfunc LoadCSV(logger *slog.Logger, filePath string) ([]profiler.Column, string, error) {\n    if logger == nil {\n        logger = slog.New(slog.NewJSONHandler(io.Discard, nil))\n    }\n    file, err := os.Open(filePath)\n    if err != nil {\n        logger.Error(\"Falha ao abrir arquivo\", \"path\", filePath, \"error\", err)\n        return nil, \"\", err\n    }\n    defer file.Close()\n\n    nameFile := filepath.Base(file.Name())\n    column, err := ParseData(logger, file)\n    return column, nameFile, err\n}\n\nfunc ParseData(logger *slog.Logger, file io.Reader) ([]profiler.Column, error) {\n    if logger == nil {\n        logger = slog.New(slog.NewJSONHandler(io.Discard, nil))\n    }\n    smartReader, err := NewSmartReader(logger, file)\n    if err != nil {\n        return nil, err\n    }\n\n    bufferedSmartReader := bufio.NewReaderSize(smartReader, 1024*1024)\n\n    separator, err := DetectSeparator(bufferedSmartReader)\n    if err != nil {\n        separator = ';'\n        logger.Warn(\"Falha na detec\u00e7\u00e3o de separador, usando fallback\", \"error\", err, \"fallback\", separator)\n    } else {\n        logger.Info(\"Separador detectado\", \"separator\", string(separator))\n    }\n\n    reader := csv.NewReader(bufferedSmartReader)\n    reader.Comma = separator\n    reader.LazyQuotes = true\n    records, err := reader.ReadAll()\n    if err != nil {\n        return nil, err\n    }\n\n    if len(records) == 0 {\n        return []profiler.Column{}, nil\n    }\n\n    headers := records[0]\n\n    logger.Info(\"Estrutura carregada\",\n        \"total_rows\", len(records),\n        \"columns_count\", len(headers),\n        \"headers\", headers,\n    )\n    columns := make([]profiler.Column, len(headers))\n    for i, name := range headers {\n        columns[i] = profiler.Column{\n            Name:   name,\n            Values: make([]string, 0, len(records)-1),\n        }\n    }\n\n    for _, row := range records[1:] {\n        for i, value := range row {\n            if i &lt; len(columns) {\n                columns[i].Values = append(columns[i].Values, value)\n            }\n        }\n    }\n\n    return columns, nil\n}\n\nfunc ParseDataAsync(ctx context.Context, logger *slog.Logger, r io.Reader) ([]string, &lt;-chan profiler.StreamData, error) {\n    if logger == nil {\n        logger = slog.New(slog.NewJSONHandler(io.Discard, nil))\n    }\n\n    smartReader, err := NewSmartReader(logger, r)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    bufferedSmartReader := bufio.NewReaderSize(smartReader, 1024*1024)\n    isJson, err := sniffJSON(bufferedSmartReader)\n    if err != nil {\n        return nil, nil, fmt.Errorf(\"erro ao detectar formato: %w\", err)\n    }\n\n    if isJson {\n        logger.Info(\"Formato detectado: JSONL (Logs/NoSQL)\")\n        return parseJSONLAsync(ctx, logger, bufferedSmartReader)\n    }\n    logger.Info(\"Formato detectado: CSV (Tabular)\")\n    return parseCSVAsync(ctx, logger, bufferedSmartReader)\n\n}\n\nfunc sniffJSON(r *bufio.Reader) (bool, error) {\n    bytesToPeek, err := r.Peek(50)\n    if err != nil &amp;&amp; err != io.EOF {\n        return false, err\n    }\n\n    for _, b := range bytesToPeek {\n        if unicode.IsSpace(rune(b)) {\n            continue\n        }\n        if b == '{' {\n            return true, nil\n        }\n        return false, nil\n    }\n    return false, nil\n}\n\nfunc parseCSVAsync(ctx context.Context, logger *slog.Logger, reader *bufio.Reader) ([]string, &lt;-chan profiler.StreamData, error) {\n    out := make(chan profiler.StreamData, 1000)\n\n    separator, err := DetectSeparator(reader)\n    if err != nil {\n        separator = ';'\n        logger.Warn(\"Falha na detec\u00e7\u00e3o de separador, usando fallback\", \"error\", err, \"fallback\", separator)\n    } else {\n        logger.Info(\"Separador detectado\", \"separator\", string(separator))\n    }\n\n    csvReader := csv.NewReader(reader)\n    csvReader.Comma = separator\n    csvReader.LazyQuotes = true\n    csvReader.ReuseRecord = true\n\n    headersRef, err := csvReader.Read()\n    if err != nil {\n        close(out)\n        return nil, nil, err\n    }\n    headers := make([]string, len(headersRef))\n    copy(headers, headersRef)\n    csvReader.FieldsPerRecord = len(headers)\n    logger.Info(\"In\u00edcio do streaming\",\n        \"columns_count\", len(headers),\n        \"headers\", headers,\n    )\n\n    go func() {\n        defer close(out)\n        count := 0\n        lineNum := 1\n        errorCount := 0\n        for {\n            select {\n            case &lt;-ctx.Done():\n                logger.Warn(\"Leitura cancelada pelo contexto\")\n                return\n            default:\n                record, err := csvReader.Read()\n                lineNum++\n                if err == io.EOF {\n                    goto EndProcessing\n                }\n                if err != nil {\n                    errorCount++\n                    out &lt;- profiler.StreamData{\n                        Row:        nil,\n                        LineNumber: lineNum,\n                        Err:        err,\n                    }\n                    continue\n                }\n\n                rowCopy := profiler.GetRowSlice()\n                rowCopy = append(rowCopy, record...)\n\n                out &lt;- profiler.StreamData{\n                    Row:        rowCopy,\n                    LineNumber: lineNum,\n                    Err:        nil,\n                }\n                count++\n\n            }\n        }\n    EndProcessing:\n        logger.Info(\"Streaming CSV finalizado\",\n            \"total_rows_read\", lineNum-1,\n            \"total_errors\", errorCount,\n        )\n    }()\n\n    return headers, out, nil\n}\n\nfunc parseJSONLAsync(ctx context.Context, logger *slog.Logger, reader *bufio.Reader) ([]string, &lt;-chan profiler.StreamData, error) {\n    out := make(chan profiler.StreamData, 1000)\n\n    scanner := bufio.NewScanner(reader)\n\n    const maxCapacity = 1024 * 1024\n    buf := make([]byte, maxCapacity)\n    scanner.Buffer(buf, maxCapacity)\n\n    if !scanner.Scan() {\n        if err := scanner.Err(); err != nil {\n            return nil, nil, fmt.Errorf(\"erro lendo primeira linha JSON: %w\", err)\n        }\n        return nil, nil, errors.New(\"arquivo JSONL vazio\")\n    }\n\n    firstLine := scanner.Bytes()\n    var firstMap map[string]interface{}\n    if err := json.Unmarshal(firstLine, &amp;firstMap); err != nil {\n        return nil, nil, fmt.Errorf(\"erro de parsing na primeira linha (n\u00e3o \u00e9 JSON v\u00e1lido?): %w\", err)\n    }\n\n    headers := make([]string, 0, len(firstMap))\n    for k := range firstMap {\n        headers = append(headers, k)\n    }\n\n    sort.Strings(headers)\n\n    logger.Info(\"Schema JSONL inferido\", \"headers\", headers)\n\n    go func() {\n        defer close(out)\n\n        processMap := func(m map[string]interface{}, lineNum int) {\n            row := profiler.GetRowSlice()\n\n            for _, header := range headers {\n                val, exists := m[header]\n                if !exists || val == nil {\n                    row = append(row, \"\")\n                } else {\n                    row = append(row, fmt.Sprintf(\"%v\", val))\n                }\n            }\n\n            out &lt;- profiler.StreamData{\n                Row:        row,\n                LineNumber: lineNum,\n                Err:        nil,\n            }\n        }\n\n        processMap(firstMap, 1)\n\n        lineNum := 1\n        for scanner.Scan() {\n            select {\n            case &lt;-ctx.Done():\n                return\n            default:\n            }\n            lineNum++\n\n            if len(scanner.Bytes()) == 0 {\n                continue\n            }\n\n            var currentMap map[string]interface{}\n            if err := json.Unmarshal(scanner.Bytes(), &amp;currentMap); err != nil {\n                out &lt;- profiler.StreamData{\n                    LineNumber: lineNum,\n                    Err:        fmt.Errorf(\"json malformado: %w\", err),\n                }\n                continue\n            }\n\n            processMap(currentMap, lineNum)\n        }\n\n        if err := scanner.Err(); err != nil {\n            logger.Error(\"Erro fatal no scanner JSONL\", \"error\", err)\n            out &lt;- profiler.StreamData{\n                LineNumber: lineNum,\n                Err:        fmt.Errorf(\"erro de I/O: %w\", err),\n            }\n        }\n    }()\n\n    return headers, out, nil\n}\n\nfunc NewSmartReader(logger *slog.Logger, r io.Reader) (io.Reader, error) {\n    br := bufio.NewReaderSize(r, 1024*1024)\n\n    bomCheck, err := br.Peek(4)\n    if err != nil &amp;&amp; err != io.EOF &amp;&amp; len(bomCheck) &lt; 2 {\n        return br, nil\n    }\n\n    if len(bomCheck) &gt;= 2 &amp;&amp; bomCheck[0] == 0xFF &amp;&amp; bomCheck[1] == 0xFE {\n        logger.Info(\"Encoding detectado: UTF-16 LE (Convertendo para UTF-8)\")\n        win16le := unicodeenc.UTF16(unicodeenc.LittleEndian, unicodeenc.UseBOM)\n        return transform.NewReader(br, win16le.NewDecoder()), nil\n    }\n    if len(bomCheck) &gt;= 2 &amp;&amp; bomCheck[0] == 0xFE &amp;&amp; bomCheck[1] == 0xFF {\n        logger.Info(\"Encoding detectado: UTF-16 BE (Convertendo para UTF-8)\")\n        win16be := unicodeenc.UTF16(unicodeenc.BigEndian, unicodeenc.UseBOM)\n        return transform.NewReader(br, win16be.NewDecoder()), nil\n    }\n\n    const sampleSize = 2048\n    sample, err := br.Peek(sampleSize)\n\n    if err != nil &amp;&amp; err != io.EOF {\n        return nil, err\n    }\n\n    if utf8.Valid(sample) {\n        logger.Debug(\"Encoding detectado: UTF-8 (Nativo)\")\n        return br, nil\n    }\n\n    logger.Warn(\"Encoding UTF-8 inv\u00e1lido detectado na amostra. Aplicando fallback Windows1252 -&gt; UTF-8\")\n    decoderReader := transform.NewReader(br, charmap.Windows1252.NewDecoder())\n    return decoderReader, nil\n}\n\nfunc DetectSeparator(r *bufio.Reader) (rune, error) {\n\n    bytesToPeek, err := r.Peek(2048)\n    if err != nil &amp;&amp; err != io.EOF {\n        return ';', err\n    }\n\n    semicolonCount := 0 // ;\n    commaCount := 0     // ,\n    pipeCount := 0      // |\n    tabCount := 0       // \\t\n\n    for _, b := range bytesToPeek {\n        if b == '\\n' || b == '\\r' {\n            break\n        }\n        switch b {\n        case ';':\n            semicolonCount++\n        case ',':\n            commaCount++\n        case '|':\n            pipeCount++\n        case '\\t':\n            tabCount++\n        }\n    }\n\n    separator := ';'\n    maxCount := semicolonCount\n\n    if commaCount &gt; maxCount {\n        maxCount = commaCount\n        separator = ','\n    }\n    if pipeCount &gt; maxCount {\n        maxCount = pipeCount\n        separator = '|'\n    }\n    if tabCount &gt; maxCount {\n        separator = '\\t'\n    }\n\n    return separator, nil\n}\n</code></pre>"},{"location":"engineering/backend/#5-api-e-server-sent-events-sse","title":"5. API e Server-Sent Events (SSE)","text":"<p>A comunica\u00e7\u00e3o com o frontend n\u00e3o bloqueia a requisi\u00e7\u00e3o de upload.</p> <ol> <li>Upload Handler: Recebe o arquivo e despacha uma goroutine <code>go p.Run()</code>.</li> <li>SSE Handler: Mant\u00e9m uma conex\u00e3o HTTP aberta para enviar o progresso calculado pelo <code>ProgressTracker</code>.</li> </ol> <p>Graceful Shutdown</p> <p>O servidor utiliza <code>context.Context</code> para cancelar o processamento de arquivos caso o cliente encerre a conex\u00e3o abruptamente, liberando recursos do servidor.</p> cmd/api/main.go<pre><code>    logger := slog.New(slog.NewJSONHandler(logOutput, nil))\n\n    slog.SetDefault(logger)\n\n    if *cliMode {\n        if *filePath == \"\" {\n            slog.Error(\"Erro: No modo -cli, forne\u00e7a o arquivo: -file=\\\"dados.csv\\\"\")\n            os.Exit(1)\n        }\n        runCLI(logger, *filePath)\n        return\n    }\n\n    runServer()\n\n}\n\nfunc runCLI(logger *slog.Logger, path string) {\n    start := time.Now()\n\n    logger.Info(\"CLI: Iniciando DataProfiler\", \"mode\", \"streaming\", \"file\", path)\n</code></pre> <p>(Exemplo de trecho do main.go configurando as rotas)</p>"},{"location":"engineering/backend/arquitetura/","title":"Arquitetura de Streaming e Performance","text":""},{"location":"engineering/backend/arquitetura/#arquitetura-de-streaming-e-performance","title":"Arquitetura de Streaming e Performance","text":"<p>Esta se\u00e7\u00e3o descreve as decis\u00f5es de engenharia que permitem ao DataProfiler processar arquivos massivos (GBs) mantendo uma pegada de mem\u00f3ria m\u00ednima (MBs). A arquitetura resolve o problema cl\u00e1ssico de Big Data em Hardware Pequeno.</p>"},{"location":"engineering/backend/arquitetura/#1-o-desafio-de-engenharia","title":"\ud83c\udfaf 1. O Desafio de Engenharia","text":"<p>Em abordagens tradicionais de Ci\u00eancia de Dados (como Python/Pandas ou R), o padr\u00e3o \u00e9 carregar todo o dataset na mem\u00f3ria RAM (In-Memory Processing).</p> <p>Cen\u00e1rio Tradicional</p> <ul> <li>Arquivo de entrada: CSV de 10 GB</li> <li>Infraestrutura: Container serverless (Render, AWS Lambda) com 512 MB de RAM</li> <li>Resultado: Processo encerrado com erro <code>OOMKilled</code> antes da an\u00e1lise come\u00e7ar</li> </ul> <p>Abordagem DataProfiler</p> <p>Adotamos uma arquitetura de Streaming Pipeline. Em vez de carregar o dataset inteiro, os dados s\u00e3o tratados como um fluxo cont\u00ednuo:</p> <p>L\u00ea \u2192 Processa \u2192 Descarta da mem\u00f3ria</p>"},{"location":"engineering/backend/arquitetura/#2-pipeline-de-processamento","title":"\ud83d\udd04 2. Pipeline de Processamento","text":"<p>O fluxo de dados segue o padr\u00e3o Producer\u2013Consumer, utilizando as primitivas de concorr\u00eancia do Go (<code>Channels</code> e <code>Goroutines</code>).</p> <pre><code>graph LR\n    User[Usu\u00e1rio] --&gt;|Upload HTTP Multipart| Server[Servidor Go]\n\n    subgraph \"Camada de Ingest\u00e3o (I/O)\"\n        Server --&gt;|Stream 32MB chunks| Disk[Disco Tempor\u00e1rio]\n        Disk --&gt;|Buffer 1MB| Reader[Leitor Bufio]\n        Reader --&gt;|Parse CSV| Parser[CSV Parser]\n    end\n\n    subgraph \"Camada de Processamento (Concorr\u00eancia)\"\n        Parser --&gt;|Envia Linha| Chan{Channel Buffer: 1000}\n\n        Chan --&gt;|Consome| W1[Worker 1: Infer\u00eancia]\n        Chan --&gt;|Consome| W2[Worker 2: Estat\u00edstica]\n        Chan --&gt;|Consome| W3[Worker 3: Regex PII]\n    end\n\n    subgraph \"Camada de Agrega\u00e7\u00e3o\"\n        W1 &amp; W2 &amp; W3 --&gt;|Resultados Parciais| Agg[Acumulador]\n        Agg --&gt;|JSON Final| UI[Frontend React]\n    end</code></pre>"},{"location":"engineering/backend/arquitetura/#3-tuning-de-performance","title":"\u2699\ufe0f 3. Tuning de Performance","text":""},{"location":"engineering/backend/arquitetura/#os-numeros-magicos","title":"Os \u201cN\u00fameros M\u00e1gicos\u201d","text":"<p>A efici\u00eancia do sistema depende do ajuste preciso de buffers e limites. Abaixo est\u00e3o as principais decis\u00f5es t\u00e9cnicas.</p>"},{"location":"engineering/backend/arquitetura/#31-otimizacao-de-io-de-disco-bufio","title":"\ud83d\udcc0 3.1 Otimiza\u00e7\u00e3o de I/O de Disco (<code>bufio</code>)","text":"<p>Ler dados do disco \u00e9 uma opera\u00e7\u00e3o lenta. A leitura byte a byte gera milh\u00f5es de syscalls, degradando a performance.</p> Implementa\u00e7\u00e3oDecis\u00e3o T\u00e9cnica <p>bufio.NewReaderSize(file, 1024*1024)</p> <ul> <li>Buffer de 1 MB</li> <li>Reduz drasticamente o n\u00famero de acessos ao disco</li> <li>Aumenta o throughput de leitura</li> </ul>"},{"location":"engineering/backend/arquitetura/#32-limite-de-upload-multipart-form","title":"\ud83d\udce4 3.2 Limite de Upload (Multipart Form)","text":"<p>Uploads grandes podem esgotar a mem\u00f3ria do servidor se n\u00e3o houver controle.</p> Implementa\u00e7\u00e3oDecis\u00e3o T\u00e9cnica <pre><code>r.ParseMultipartForm(32 &lt;&lt; 20)\n</code></pre> <ul> <li>Apenas 32 MB permanecem em RAM</li> <li>O excedente \u00e9 automaticamente escrito em disco</li> <li>Protege o servidor contra uploads de v\u00e1rios GBs</li> </ul>"},{"location":"engineering/backend/arquitetura/#33-backpressure-channel-buffering","title":"\ud83d\udea6 3.3 Backpressure (Channel Buffering)","text":"<p>A leitura de disco \u00e9 mais r\u00e1pida que o processamento em CPU. Sem controle, isso pode gerar ac\u00famulo excessivo de dados na mem\u00f3ria.</p> Implementa\u00e7\u00e3oDecis\u00e3o T\u00e9cnica <p>jobs := make(chan []string, 1000)</p> <ul> <li>Channel com buffer de 1000 linhas</li> <li>Quando o buffer enche:</li> <li>O leitor de disco \u00e9 automaticamente bloqueado</li> <li>Cria backpressure natural, equilibrando I/O e CPU</li> </ul> <p>Resultado</p> <p>O sistema se auto-regula conforme a velocidade do processamento, garantindo estabilidade e previsibilidade.</p>"},{"location":"engineering/backend/arquitetura/#4-concorrencia-worker-pool","title":"\ud83e\uddf5 4. Concorr\u00eancia: Worker Pool","text":"<p>As goroutines do Go s\u00e3o extremamente leves (~2 KB), muito menores que threads do sistema operacional (~1 MB).</p> <p>Estrat\u00e9gia</p> <ul> <li>Um n\u00famero fixo de workers \u00e9 iniciado (baseado no n\u00famero de CPUs)</li> <li>Todos consomem linhas do mesmo channel</li> <li>Se um worker ficar lento (ex.: regex pesada), os outros continuam processando Benef\u00edcio: \u2714\ufe0f Melhor uso da CPU \u2714\ufe0f Paralelismo real \u2714\ufe0f Alta escalabilidade com baixo consumo de mem\u00f3ria</li> </ul>"},{"location":"engineering/backend/arquitetura/#5-distribuicao-binario-unico-embed","title":"\ud83d\udce6 5. Distribui\u00e7\u00e3o: Bin\u00e1rio \u00danico (Embed)","text":"<p>Para simplificar o deploy e eliminar depend\u00eancias externas (Nginx/Apache), utilizamos o <code>embed</code> do Go (v1.16+).</p> Como FuncionaBenef\u00edcios <p>Durante o <code>go build</code>, os arquivos est\u00e1ticos do frontend (React) s\u00e3o embutidos diretamente no bin\u00e1rio.     //go:embed frontend/dist/*     var frontendFS embed.FS</p> <ul> <li>Um \u00fanico arquivo execut\u00e1vel</li> <li>Cont\u00e9m:</li> <li>API</li> <li>Pipeline de processamento</li> <li>Interface Web</li> <li>Deploy simples, port\u00e1til e previs\u00edvel</li> </ul> <p>Resumo Final</p> <p>O DataProfiler combina streaming, concorr\u00eancia eficiente e deploy simplificado para processar Big Data em ambientes com recursos extremamente limitados.</p>"},{"location":"engineering/frontend/","title":"Engenharia Frontend","text":""},{"location":"engineering/frontend/#engenharia-frontend","title":"Engenharia Frontend","text":"<p>A interface do Data Profiler \u00e9 uma Single Page Application (SPA) moderna, constru\u00edda para oferecer feedback visual instant\u00e2neo durante o processamento de grandes volumes de dados.</p> <p>A arquitetura prioriza a densidade de informa\u00e7\u00e3o (via DataGrid) e a reatividade (via Server-Sent Events), eliminando a necessidade de polling constante.</p>"},{"location":"engineering/frontend/#1-stack-tecnologica","title":"1. Stack Tecnol\u00f3gica","text":"<p>As decis\u00f5es t\u00e9cnicas visam performance de renderiza\u00e7\u00e3o e estabilidade.</p> Tecnologia Vers\u00e3o Fun\u00e7\u00e3o React <code>19.x</code> Biblioteca de UI principal. Vite <code>7.x</code> Build tool e Hot Module Replacement (HMR). MUI (Material UI) <code>7.x</code> Design System Enterprise (componentes prontos). Recharts <code>3.x</code> Visualiza\u00e7\u00e3o de dados estat\u00edsticos. React Error Boundary <code>6.x</code> Tratamento global de falhas na UI. <p>Performance de Build</p> <p>O projeto utiliza SWC (Rust) no lugar do Babel para transpila\u00e7\u00e3o, reduzindo o tempo de build em ambientes de CI/CD.</p>"},{"location":"engineering/frontend/#2-arquitetura-de-componentes","title":"2. Arquitetura de Componentes","text":"<p>A aplica\u00e7\u00e3o segue uma hierarquia de componentes funcionais. O estado global \u00e9 minimizado em favor de estados locais e composi\u00e7\u00e3o.</p> <pre><code>graph TD\n    Root[main.jsx] --&gt; ErrorBoundary[GlobalErrorFallback]\n    ErrorBoundary --&gt; App[App.jsx]\n\n    subgraph \"Fluxo de Upload\"\n        App --&gt; Uploader[FileUploader]\n        Uploader --&gt;|Inicia SSE| Progress[UploadProgress]\n    end\n\n    subgraph \"Visualiza\u00e7\u00e3o\"\n        App --&gt; Report[DataReport]\n        Report --&gt; Stats[ColumnAnalysisCard]\n        Report --&gt; Grid[DataPreviewTable]\n        Report --&gt; Charts[Recharts]\n    end\n</code></pre> Figura 1: Fluxo dentro do frontend"},{"location":"engineering/frontend/#3-comunicacao-real-time-sse","title":"3. Comunica\u00e7\u00e3o Real-Time (SSE)","text":"<p>Para evitar sobrecarga no servidor com requisi\u00e7\u00f5es HTTP repetidas (polling), utilizamos Server-Sent Events (SSE).</p> <ol> <li>O cliente faz POST em <code>/upload</code>.</li> <li>O servidor retorna <code>202 Accepted</code> imediatamente.</li> <li>O cliente abre conex\u00e3o em <code>/events</code>.</li> <li>O servidor envia atualiza\u00e7\u00f5es de progresso (0-100%) via stream de texto.</li> </ol> <p>Conex\u00f5es Persistentes</p> <p>O frontend deve gerenciar o ciclo de vida do <code>EventSource</code>, fechando a conex\u00e3o explicitamente ao receber o evento de <code>complete</code> ou <code>error</code> para evitar vazamento de recursos no navegador.</p>"},{"location":"engineering/frontend/#4-integracao-single-binary-embed","title":"4. Integra\u00e7\u00e3o Single Binary (Embed)","text":"<p>O frontend n\u00e3o \u00e9 servido por um servidor Node.js separado em produ\u00e7\u00e3o. Ele \u00e9 compilado e embutido dentro do bin\u00e1rio Go.</p> <p>O pacote <code>frontend</code> utiliza a diretiva <code>embed</code> do Go 1.16+ para virtualizar o sistema de arquivos da pasta <code>dist</code>.</p> frontend/embed.go<pre><code>package frontend\n\nimport (\n    \"embed\"\n    \"io/fs\"\n    \"net/http\"\n)\n\n// A diretiva abaixo diz ao Go: \"Inclua a pasta 'dist' inteira dentro do bin\u00e1rio\".\n// Como este arquivo est\u00e1 DENTRO da pasta frontend, o caminho relativo \u00e9 apenas \"dist\".\n//\n//go:embed dist/*\nvar distFS embed.FS\n\n// GetFileSystem retorna o sistema de arquivos pronto para o servidor web.\n// Ele j\u00e1 faz o \"Sub\" para entrar na pasta dist, assim o servidor acha o index.html na raiz.\nfunc GetFileSystem() (http.FileSystem, error) {\n    // Entra na subpasta \"dist\" do sistema de arquivos embutido\n    fsys, err := fs.Sub(distFS, \"dist\")\n    if err != nil {\n        return nil, err\n    }\n    return http.FS(fsys), nil\n}\n</code></pre>"},{"location":"engineering/frontend/#processo-de-build","title":"Processo de Build","text":"<ol> <li>Frontend: <code>npm run build</code> gera os arquivos est\u00e1ticos na pasta <code>dist/</code>.</li> <li>Go: O compilador l\u00ea a diretiva <code>//go:embed dist/*</code>.</li> <li>Resultado: Um \u00fanico execut\u00e1vel <code>.exe</code> cont\u00e9m toda a aplica\u00e7\u00e3o React, HTML, CSS e imagens.</li> </ol>"},{"location":"engineering/frontend/#5-padroes-de-codigo-e-linting","title":"5. Padr\u00f5es de C\u00f3digo e Linting","text":"<p>O projeto utiliza ESLint com configura\u00e7\u00e3o \"Flat Config\" para garantir a qualidade do c\u00f3digo JavaScript/JSX.</p> eslint.config.js<pre><code>    ],\n    languageOptions: {\n      ecmaVersion: 2020,\n      globals: globals.browser,\n      parserOptions: {\n        ecmaVersion: 'latest',\n        ecmaFeatures: { jsx: true },\n        sourceType: 'module',\n      },\n</code></pre>"},{"location":"guide/","title":"Guia de Instala\u00e7\u00e3o e Execu\u00e7\u00e3o","text":""},{"location":"guide/#guia-de-instalacao-e-execucao","title":"Guia de Instala\u00e7\u00e3o e Execu\u00e7\u00e3o","text":"<p>O DataProfiler foi projetado para ser agn\u00f3stico de plataforma. Oferecemos tr\u00eas m\u00e9todos de execu\u00e7\u00e3o, variando do \"clique e use\" (para usu\u00e1rios finais) at\u00e9 a compila\u00e7\u00e3o total (para desenvolvedores).</p>"},{"location":"guide/#metodo-1-executavel-recomendado","title":"M\u00e9todo 1: Execut\u00e1vel (Recomendado)","text":"<p>A forma mais simples de utilizar o DataProfiler \u00e9 atrav\u00e9s do conceito de Single Binary. O Backend (Go) e o Frontend (React) foram compilados em um \u00fanico arquivo. N\u00e3o \u00e9 necess\u00e1rio instalar Java, Python ou Node.js.</p>"},{"location":"guide/#windows","title":"Windows","text":"<ol> <li>Acesse a P\u00e1gina de Releases do projeto.</li> <li>Baixe o arquivo <code>dataprofiler-windows-amd64.exe</code>.</li> <li>D\u00ea um duplo clique no arquivo baixado.</li> <li>O terminal se abrir\u00e1 e, em seguida, seu navegador padr\u00e3o abrir\u00e1 automaticamente em <code>http://localhost:8080</code>.</li> </ol> <p>Alerta do Windows Defender</p> <p>Como este \u00e9 um software open-source n\u00e3o assinado digitalmente (o que custa caro), o Windows pode exibir a tela \"O Windows protegeu o computador\".</p> <p>Isso \u00e9 um falso positivo. Para prosseguir:</p> <ol> <li>Clique em \"Mais informa\u00e7\u00f5es\".</li> <li>Clique no bot\u00e3o \"Executar assim mesmo\".</li> </ol>"},{"location":"guide/#linux-macos","title":"Linux / macOS","text":"<ol> <li>Acesse a P\u00e1gina de Releases do projeto.</li> <li>Baixe o arquivo <code>dataprofiler-linux-amd64</code> (ou <code>darwin</code> para Mac).</li> <li>Abra o terminal na pasta do download e d\u00ea permiss\u00e3o de execu\u00e7\u00e3o:</li> </ol> <pre><code>chmod +x dataprofiler-linux-amd64\n</code></pre> <ol> <li>Execute o programa:</li> </ol> <pre><code>./dataprofiler-linux-amd64\n</code></pre>"},{"location":"guide/#metodo-2-docker-ambiente-isolado","title":"M\u00e9todo 2: Docker (Ambiente Isolado)","text":"<p>Se voc\u00ea prefere n\u00e3o rodar bin\u00e1rios diretamente no seu sistema operacional, disponibilizamos uma imagem Docker oficial. Este m\u00e9todo garante que o ambiente seja id\u00eantico ao de produ\u00e7\u00e3o.</p> <p>Pr\u00e9-requisitos: Docker e Docker Compose instalados.</p> <p>Crie um arquivo <code>docker-compose.yml</code>:</p> <pre><code>title=\"docker-compose.yml\"\nversion: \"3.8\"\nservices:\n  app:\n    image: jgustavocn/dataprofiler:latest\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./uploads:/app/uploads\n</code></pre> <p>No terminal, execute:</p> <pre><code>docker compose up -d\n</code></pre> <p>O sistema estar\u00e1 dispon\u00edvel em: http://localhost:8080</p>"},{"location":"guide/#metodo-3-compilando-do-codigo-para-desenvolvedores","title":"M\u00e9todo 3: Compilando do C\u00f3digo (Para Desenvolvedores)","text":"<p>Se voc\u00ea deseja contribuir com o c\u00f3digo ou testar funcionalidades experimentais, siga os passos de compila\u00e7\u00e3o manual.</p>"},{"location":"guide/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Go: Vers\u00e3o 1.22 ou superior.</li> <li>Node.js: Vers\u00e3o 20 (LTS) ou superior.</li> <li>Make: (Opcional, mas recomendado).</li> </ul>"},{"location":"guide/#passo-a-passo","title":"Passo a Passo","text":"<ol> <li>Clone o reposit\u00f3rio:</li> </ol> <pre><code>git clone https://github.com/JGustavoCN/dataprofiler.git\ncd dataprofiler\n</code></pre> <ol> <li>Instale as depend\u00eancias (Backend e Frontend):    Utilizamos o Makefile para automatizar a instala\u00e7\u00e3o das libs do Go e os pacotes npm do React.</li> </ol> <pre><code>make setup\n</code></pre> <ol> <li>Execute em modo de desenvolvimento:    Este comando roda o Backend com Hot-Reload (Air) e o Frontend em modo dev.</li> </ol> <pre><code>make run\n</code></pre>"},{"location":"guide/#troubleshooting-resolucao-de-problemas","title":"\ud83d\udd27 Troubleshooting (Resolu\u00e7\u00e3o de Problemas)","text":""},{"location":"guide/#erro-address-already-in-use-porta-8080-ocupada","title":"Erro: \"Address already in use\" (Porta 8080 ocupada)","text":"<p>Se voc\u00ea j\u00e1 tiver outro servi\u00e7o rodando na porta <code>8080</code> (comum em desenvolvedores Java/Tomcat), o DataProfiler n\u00e3o iniciar\u00e1.</p> <p>Solu\u00e7\u00e3o: Defina a vari\u00e1vel de ambiente <code>PORT</code> antes de executar.</p>"},{"location":"guide/#windows-powershell","title":"Windows (PowerShell)","text":"<pre><code>$env:PORT=\"9090\"; .\\dataprofiler.exe\n</code></pre>"},{"location":"guide/#linux-mac","title":"Linux / Mac","text":"<pre><code>PORT=9090 ./dataprofiler\n</code></pre>"},{"location":"guide/#erro-tela-branca-ou-connection-refused","title":"Erro: Tela Branca ou \"Connection Refused\"","text":"<p>Certifique-se de que o backend Go est\u00e1 rodando. O Frontend React depende da API para funcionar. Se voc\u00ea estiver rodando via c\u00f3digo fonte, garanta que ambos os terminais (Go e Node) estejam ativos.</p>"},{"location":"guide/funcionalidades/","title":"Regras de Neg\u00f3cio e Funcionalidades","text":""},{"location":"guide/funcionalidades/#regras-de-negocio-e-funcionalidades","title":"Regras de Neg\u00f3cio e Funcionalidades","text":"<p>O diferencial do DataProfiler \u00e9 sua capacidade de \"entender\" o dado, n\u00e3o apenas l\u00ea-lo. Abaixo detalhamos os algoritmos de infer\u00eancia.</p>"},{"location":"guide/funcionalidades/#1-calculo-de-sla-nivel-de-qualidade","title":"1. C\u00e1lculo de SLA (N\u00edvel de Qualidade)","text":"<p>O sistema atribui um selo de qualidade para cada coluna processada. Isso permite que engenheiros de dados decidam rapidamente se aquela coluna pode ser usada em um modelo de Machine Learning ou Dashboard.</p>"},{"location":"guide/funcionalidades/#a-logica-matematica","title":"A L\u00f3gica Matem\u00e1tica","text":"<p>O c\u00e1lculo \u00e9 baseado na Densidade de Informa\u00e7\u00e3o. O sistema contabiliza, em tempo real, quantos valores s\u00e3o considerados \"sujos\" (Nulos, Vazios, <code>NA</code>, <code>NULL</code>).</p> \\[\\text{Score} = \\frac{\\text{Total Linhas} - \\text{Linhas Sujas}}{\\text{Total Linhas}} \\times 100\\]"},{"location":"guide/funcionalidades/#classificacao","title":"Classifica\u00e7\u00e3o","text":"Selo Crit\u00e9rio Interpreta\u00e7\u00e3o \ud83e\udd47 Ouro Score \u2265 99% Alta Confiabilidade. Dados praticamente \u00edntegros. Seguros para chaves prim\u00e1rias ou m\u00e9tricas financeiras. \ud83e\udd48 Prata 95% \u2264 Score &lt; 99% Confiabilidade M\u00e9dia. Dados \u00fateis para an\u00e1lises de tend\u00eancia, mas requerem aten\u00e7\u00e3o em casos de borda. \ud83e\udd49 Bronze Score &lt; 95% Baixa Qualidade. Requer tratamento (imputa\u00e7\u00e3o de dados) antes do uso. Alto risco de vi\u00e9s."},{"location":"guide/funcionalidades/#2-deteccao-de-sensibilidade-lgpdgdpr","title":"2. Detec\u00e7\u00e3o de Sensibilidade (LGPD/GDPR)","text":"<p>Para garantir conformidade com leis de prote\u00e7\u00e3o de dados, o DataProfiler escaneia o conte\u00fado em busca de PII (Personally Identifiable Information).</p> <p>O algoritmo funciona em duas camadas:</p> <ol> <li>An\u00e1lise de Cabe\u00e7alho: Verifica se o nome da coluna sugere dados sens\u00edveis (ex: \"cpf_cliente\", \"email_contato\").</li> <li>An\u00e1lise de Conte\u00fado (Regex): Verifica se os valores batem com padr\u00f5es conhecidos.</li> </ol>"},{"location":"guide/funcionalidades/#padroes-detectados","title":"Padr\u00f5es Detectados","text":"<p>Aten\u00e7\u00e3o</p> <p>Se uma coluna for marcada como Sens\u00edvel, o \u00edcone \ud83d\udee1\ufe0f aparecer\u00e1 no relat\u00f3rio. Recomenda-se aplicar hashing ou mascaramento nesses dados.</p> <ul> <li>CPF (Brasil): Valida\u00e7\u00e3o de formato <code>111.222.333-44</code> ou <code>11122233344</code>.</li> <li>E-mail: Padr\u00e3o RFC 5322 (<code>usuario@dominio.com</code>).</li> <li>Cart\u00e3o de Cr\u00e9dito: Detec\u00e7\u00e3o de sequ\u00eancias num\u00e9ricas compat\u00edveis com PANs (Luhn Algorithm check b\u00e1sico).</li> <li>Telefone: Padr\u00f5es globais E.164 e nacionais.</li> </ul>"},{"location":"guide/funcionalidades/#3-inferencia-de-tipos-polimorfismo","title":"3. Infer\u00eancia de Tipos (Polimorfismo)","text":"<p>Como o CSV \u00e9 um formato sem tipo (tudo \u00e9 texto), o DataProfiler realiza uma infer\u00eancia estat\u00edstica. Ele l\u00ea uma amostragem dos dados e tenta \"promover\" o tipo para o mais espec\u00edfico poss\u00edvel.</p> <p>Ordem de Tentativa:</p> <ol> <li>Integer: \u00c9 um n\u00famero inteiro? (ex: <code>42</code>)</li> <li>Float: \u00c9 decimal? (ex: <code>42.5</code> ou <code>42,5</code>) -&gt; Suporta ponto e v\u00edrgula como decimal.</li> <li>Boolean: \u00c9 l\u00f3gico? (ex: <code>true</code>, <code>1</code>, <code>sim</code>, <code>yes</code>)</li> <li>Date: \u00c9 data? (ex: <code>2023-01-01</code>, <code>01/01/2023</code>) -&gt; Suporta ISO8601 e BR.</li> <li>String: Se falhar em tudo, \u00e9 texto.</li> </ol>"},{"location":"management/","title":"Gest\u00e3o do Projeto e Backlog","text":""},{"location":"management/#gestao-do-projeto-e-backlog","title":"Gest\u00e3o do Projeto e Backlog","text":"<p>Este documento detalha a evolu\u00e7\u00e3o cronol\u00f3gica do Data Profiler, cobrindo desde o core matem\u00e1tico at\u00e9 a entrega da vers\u00e3o Enterprise e vis\u00f5es de futuro.</p>"},{"location":"management/#backlog-cronologico-milestone-1","title":"\ud83d\udcc5 Backlog Cronol\u00f3gico (Milestone 1)","text":""},{"location":"management/#fase-1-fundacao-infraestrutura","title":"Fase 1: Funda\u00e7\u00e3o &amp; Infraestrutura","text":"<p>Fase focada em fazer o sistema funcionar, ser resiliente e processar dados em streaming para evitar OOM (Out of Memory).</p>"},{"location":"management/#sprint-0-o-core-logico-a-matematica","title":"Sprint 0: O Core L\u00f3gico (A Matem\u00e1tica)","text":"<ul> <li> Task 0.1 (InferType): Detetive de Tipos com Regex (Int, Float, String).</li> <li> Task 0.2 (StatsCalc): Calculadora estat\u00edstica (M\u00e9dia, Min, Max).</li> <li> Task 0.3 (AnalyzeColumn): Analista s\u00edncrono para contagem de nulos.</li> </ul>"},{"location":"management/#sprint-1-o-mvp-happy-path","title":"Sprint 1: O MVP \"Happy Path\"","text":"<ul> <li> Task 1.1: Leitura de CSV com <code>ReadAll</code> (Carregamento total na RAM).</li> <li> Task 1.2: Servidor HTTP B\u00e1sico s\u00edncrono.</li> <li> Task 1.3: Frontend MVP com CSS Artesanal.</li> </ul>"},{"location":"management/#sprint-2-resiliencia-http","title":"Sprint 2: Resili\u00eancia HTTP","text":"<ul> <li> Task 2.1 (Timeouts): Configura\u00e7\u00e3o de <code>ReadTimeout</code> e <code>WriteTimeout</code> no servidor.</li> <li> Task 2.2 (CORS): Middleware para comunica\u00e7\u00e3o Frontend (5173) &lt;-&gt; Backend (8080).</li> <li> Task 2.3 (Context): Timeout l\u00f3gico para cancelamento de requests longos.</li> </ul>"},{"location":"management/#sprint-3-arquitetura-go-way-streaming","title":"Sprint 3: Arquitetura Go Way (Streaming)","text":"<ul> <li> Task 3.1 (Pipeline): Substitui\u00e7\u00e3o por leitura linha-a-linha via <code>Channels</code>.</li> <li> Task 3.2 (Accumulator): C\u00e1lculo estat\u00edstico sem hist\u00f3rico (Stream).</li> <li> Task 3.3 (Async): Processamento em background (<code>goroutines</code>).</li> <li> Task 3.4 (Sniffer): Detec\u00e7\u00e3o autom\u00e1tica de Encoding e Separadores.</li> </ul>"},{"location":"management/#fase-2-robustez-engenharia","title":"Fase 2: Robustez &amp; Engenharia","text":"<p>Esta fase transforma o script funcional em um software de engenharia robusta, focado em observabilidade e controle de recursos.</p>"},{"location":"management/#sprint-4-robustez-backend-engineering-material-server-network","title":"Sprint 4: Robustez &amp; Backend Engineering :material-server-network","text":"<p>Foco: Estabilidade e Observabilidade</p> <p>Preparar o motor para suportar carga pesada e ser audit\u00e1vel.</p> <ul> <li> Task 4.1 (Observabilidade): Migra\u00e7\u00e3o para <code>log/slog</code> e Baseline de mem\u00f3ria com <code>pprof</code>.</li> <li> Task 4.2 (Gest\u00e3o de Mem\u00f3ria): Implementa\u00e7\u00e3o de <code>sync.Pool</code> para redu\u00e7\u00e3o de GC.</li> <li> Task 4.3 (Lifecycle): Graceful Shutdown e cancelamento via <code>Context</code>.</li> <li> Task 4.4 (Fuzzing): Testes de estresse com dados aleat\u00f3rios na infer\u00eancia.</li> </ul>"},{"location":"management/#sprint-5-logica-de-negocio-dados-material-database-search","title":"Sprint 5: L\u00f3gica de Neg\u00f3cio &amp; Dados :material-database-search","text":"<p>Foco: Regras de Neg\u00f3cio Log\u00edsticas</p> <p>Implementar as regras que geram valor para o cliente (SLA, Sujeira, Idempot\u00eancia).</p> <ul> <li> Task 5.1 (Dirty Data): Tratamento de linhas irregulares e suporte a <code>.jsonl</code>.</li> <li> Task 5.2 (SLA Log\u00edstico): Regex para CEP, CNPJ, Placas e Score de Qualidade.</li> <li> Task 5.3 (Estat\u00edstica Stream): Reservoir Sampling (Preview) e Histogramas em streaming.</li> </ul>"},{"location":"management/#fase-3-experiencia-entrega","title":"Fase 3: Experi\u00eancia &amp; Entrega","text":"<p>Foco na usabilidade profissional e empacotamento para distribui\u00e7\u00e3o.</p>"},{"location":"management/#sprint-6-frontend-enterprise-material-ui-material-monitor-shimmer","title":"Sprint 6: Frontend Enterprise (Material UI) :material-monitor-shimmer","text":"<p>Foco: UX Profissional</p> <p>Substitui\u00e7\u00e3o do CSS artesanal por componentes de dados robustos.</p> <ul> <li> Task 6.1 (Valida\u00e7\u00e3o): Bloqueio de extens\u00f5es/tamanhos e Drag Zone ativa.</li> <li> Task 6.2 (Real-Time): Feedback de progresso via Server Sent Events (SSE).</li> <li> Task 6.3 (MUI Migration): Refatora\u00e7\u00e3o para Material UI e uso de <code>DataGrid</code>.</li> <li> Task 6.4 (Dashboards): Gr\u00e1ficos estat\u00edsticos e Cards de SLA visual.</li> </ul>"},{"location":"management/#sprint-7-packaging-devops-material-package-variant","title":"Sprint 7: Packaging &amp; DevOps :material-package-variant","text":"<p>Foco: Deploy em Arquivo \u00danico</p> <p>Gerar um artefato final f\u00e1cil de executar em qualquer ambiente.</p> <ul> <li> Task 7.1 (Single Binary): Embed do React dentro do bin\u00e1rio Go.</li> <li> Task 7.2 (CLI &amp; Benchmark): Modo terminal e valida\u00e7\u00e3o do desafio 10GB/512MB.</li> <li> Task 7.3 (Docker): Dockerfile Multi-stage e Makefile de automa\u00e7\u00e3o.</li> <li> Task 7.4 (Deploy): Fazer deploy do docker multi-stage do single-binary no Render.</li> <li> Task 7.5 (Documenta\u00e7\u00e3o): Fazer a organiza\u00e7\u00e3o e documenta\u00e7\u00e3o do que foi feito.</li> <li> Task 7.6 (Ghost Mode): Flag <code>windowsgui</code> para rodar como servi\u00e7o oculto.</li> </ul>"},{"location":"management/#historico-de-bugs-resolvidos","title":"\ud83d\udc1b Hist\u00f3rico de Bugs Resolvidos","text":"<p>Registro de problemas t\u00e9cnicos identificados e solucionados durante o desenvolvimento da Milestone.</p> ID Problema Solu\u00e7\u00e3o Aplicada Status Bug-01 O \"Fantasma\" do JSON: Nome do arquivo antigo persistia ap\u00f3s novo upload. Limpeza de estado <code>fileName</code> no reset do componente. Bug-02 Erro no \"Cat\u00e1logo de Cursos\": Falha ao processar CSV espec\u00edfico. Ajuste no <code>SmartReader</code> para encoding e delimitadores."},{"location":"management/#futuro-milestones-2-e-3","title":"\ud83d\udd2e Futuro (Milestones 2 e 3)","text":"<p>Backlog de itens para an\u00e1lise p\u00f3s-release da vers\u00e3o 1.0.</p>"},{"location":"management/#divida-tecnica-refatoracao","title":"\ud83d\udee0\ufe0f D\u00edvida T\u00e9cnica &amp; Refatora\u00e7\u00e3o","text":"<ul> <li> Task 8.1 (Config Central): Centralizar \"Magic Numbers\" e ENVs em pacote <code>internal/config</code>.</li> <li> Refactor (Multipart): Migrar de buffer em disco para streaming de rede (apenas se lat\u00eancia for cr\u00edtica).</li> <li> DevEx: Configurar <code>go fmt</code> e <code>go race</code> no pipeline de CI/CD.</li> </ul>"},{"location":"management/#novas-features-roadmap","title":"\u2728 Novas Features (Roadmap)","text":"<ul> <li> Persist\u00eancia: Banco de dados (SQLite/Postgres) para hist\u00f3rico de an\u00e1lises.</li> <li> Cardinalidade: Algoritmo HyperLogLog para contagem de \u00fanicos em Big Data.</li> <li> Webhooks: Notifica\u00e7\u00e3o passiva para sistemas externos.</li> <li> Exporta\u00e7\u00e3o: Gerar relat\u00f3rios em PDF/HTML est\u00e1tico.</li> </ul>"},{"location":"management/#pesquisa-inovacao","title":"\ud83e\uddea Pesquisa &amp; Inova\u00e7\u00e3o","text":"<p>Ideias de Expans\u00e3o</p> <ul> <li>M\u00f3dulo Educacional: Exibir f\u00f3rmulas estat\u00edsticas (Desvio Padr\u00e3o, Vari\u00e2ncia) ao clicar nos dados, servindo como ferramenta de ensino.</li> <li>Engenharia Reversa: Analisar padr\u00f5es de PII (Dados Sens\u00edveis) da lib Capital One DataProfiler.</li> <li>Contexto Log\u00edstico: Mapear valida\u00e7\u00f5es espec\u00edficas de transporte (Tempo Certo).</li> </ul>"},{"location":"management/arquitetura/","title":"Decis\u00e3o de Arquitetura 001: Stack de Documenta\u00e7\u00e3o","text":""},{"location":"management/arquitetura/#decisao-de-arquitetura-001-stack-de-documentacao","title":"Decis\u00e3o de Arquitetura 001: Stack de Documenta\u00e7\u00e3o","text":"<ul> <li>Status: Aceito</li> <li>Data: 06-01-2026</li> <li>Contexto: Engenharia &amp; Gest\u00e3o de Conhecimento</li> </ul>"},{"location":"management/arquitetura/#1-o-contexto-o-problema","title":"1. O Contexto (O Problema)","text":"<p>O projeto DataProfiler nasceu com um desafio at\u00edpico para softwares em fase inicial: a alta densidade de conhecimento pr\u00e9vio. Diferente de projetos que come\u00e7am com uma p\u00e1gina em branco, iniciamos o desenvolvimento com um legado de conhecimento bruto estimado em mais de 550 p\u00e1ginas de anota\u00e7\u00f5es t\u00e9cnicas, regras de neg\u00f3cio e estudos de viabilidade.</p> <p>Esse volume massivo de informa\u00e7\u00e3o trouxe riscos imediatos para a escalabilidade do projeto:</p> <ol> <li>Fragmenta\u00e7\u00e3o: O conhecimento estava disperso em arquivos de texto soltos e anota\u00e7\u00f5es pessoais, dificultando a consulta r\u00e1pida (\"Onde est\u00e1 aquela regra sobre Regex de CPF?\").</li> <li>Dessincronia: Manter a documenta\u00e7\u00e3o separada do c\u00f3digo (ex: em um Google Docs ou Word) criaria, inevitavelmente, uma documenta\u00e7\u00e3o obsoleta. \u00c0 medida que o c\u00f3digo Go evolu\u00edsse, o documento est\u00e1tico morreria.</li> <li>Complexidade de Manuten\u00e7\u00e3o: Gerenciar 550 p\u00e1ginas sem uma estrutura de navega\u00e7\u00e3o hier\u00e1rquica (Menu Lateral, Categorias) tornaria o consumo da informa\u00e7\u00e3o invi\u00e1vel para novos desenvolvedores ou stakeholders.</li> </ol>"},{"location":"management/arquitetura/#os-drivers-da-decisao-requisitos","title":"Os Drivers da Decis\u00e3o (Requisitos)","text":"<p>Para mitigar esses riscos, estabelecemos que a solu\u00e7\u00e3o de documenta\u00e7\u00e3o precisaria atender estritamente aos seguintes crit\u00e9rios de arquitetura:</p> <ul> <li>Filosofia \"Docs as Code\": A documenta\u00e7\u00e3o deve ser tratada como software. Ela deve viver no mesmo reposit\u00f3rio do c\u00f3digo fonte, passar por Code Review e ser versionada (Git).</li> <li>Versionamento (Git): Necessidade de rastrear a evolu\u00e7\u00e3o das decis\u00f5es. Se mudarmos a arquitetura do Backend amanh\u00e3, a documenta\u00e7\u00e3o deve ter um commit associado explicando o porqu\u00ea.</li> <li>Baixa Fric\u00e7\u00e3o de Escrita (Markdown): A ferramenta n\u00e3o poderia exigir conhecimentos complexos de HTML/CSS para escrever uma p\u00e1gina simples. O foco deve ser no conte\u00fado, n\u00e3o na diagrama\u00e7\u00e3o.</li> <li>Suporte a Diagramas: Devido \u00e0 complexidade dos fluxos de dados (Streaming, Channels, Goroutines), a ferramenta precisaria renderizar diagramas de arquitetura nativamente (ex: Mermaid.js) sem depender de imagens est\u00e1ticas <code>.png</code> que ficam desatualizadas.</li> </ul>"},{"location":"management/arquitetura/#2-opcoes-consideradas","title":"2. Op\u00e7\u00f5es Consideradas","text":"<p>Para a implementa\u00e7\u00e3o da estrat\u00e9gia de \"Docs as Code\", avaliamos as duas principais ferramentas l\u00edderes de mercado. A an\u00e1lise focou no equil\u00edbrio entre manutenibilidade (facilidade de manter o sistema rodando) e produtividade (velocidade para migrar as 550 p\u00e1ginas de conte\u00fado legado).</p>"},{"location":"management/arquitetura/#opcao-a-docusaurus-metafacebook","title":"Op\u00e7\u00e3o A: Docusaurus (Meta/Facebook)","text":"<p>Ferramenta baseada em React/Node.js.</p> <ul> <li>Atratividade: Como nosso Frontend (Client) utiliza React, esta seria a escolha \"natural\" para manter a stack unificada. Permite customiza\u00e7\u00e3o visual infinita, tratando p\u00e1ginas como componentes.</li> <li>O Bloqueio: Introduz complexidade acidental. O Docusaurus exige que a documenta\u00e7\u00e3o seja tratada como uma aplica\u00e7\u00e3o React completa. Para um projeto onde o foco \u00e9 organizar regras de neg\u00f3cio complexas, gastar tempo \"programando o layout da documenta\u00e7\u00e3o\" foi considerado um desvio de prioridade.</li> </ul>"},{"location":"management/arquitetura/#opcao-b-mkdocs-material-theme-escolhida","title":"Op\u00e7\u00e3o B: MkDocs + Material Theme (Escolhida)","text":"<p>Ferramenta baseada em Python, configurada via YAML.</p> <ul> <li>Atratividade: Filosofia \"Conte\u00fado Primeiro\". O motor transforma arquivos Markdown puros em um site est\u00e1tico sem necessidade de codifica\u00e7\u00e3o HTML/JS.</li> <li>O Diferencial: O tema Material for MkDocs oferece nativamente recursos de UX avan\u00e7ados (Busca Instant\u00e2nea, Dark Mode, Navega\u00e7\u00e3o em Abas) que no Docusaurus exigiriam configura\u00e7\u00e3o manual ou plugins.</li> </ul>"},{"location":"management/arquitetura/#criterios-de-desempate-matriz-de-decisao","title":"Crit\u00e9rios de Desempate (Matriz de Decis\u00e3o)","text":"Crit\u00e9rio Docusaurus (React) MkDocs (Python) Vencedor para o DataProfiler Configura\u00e7\u00e3o Imperativa (Code/JSX) Declarativa (YAML) MkDocs Curva de Aprendizado M\u00e9dia (Exige saber React) Baixa (Apenas Markdown) MkDocs Diagramas Plugins externos Nativo (Mermaid.js via Superfences) MkDocs Objetivo Criar portais customizados Organizar conhecimento denso MkDocs <p>Veredito: A escolha pelo MkDocs foi t\u00e9cnica e estrat\u00e9gica. Dado o volume de 550+ t\u00f3picos, a prioridade absoluta \u00e9 a velocidade de escrita e organiza\u00e7\u00e3o hier\u00e1rquica, sacrificando a liberdade criativa de design em prol de uma estrutura r\u00edgida, por\u00e9m funcional e padronizada.</p>"},{"location":"management/arquitetura/#3-a-solucao-tecnica-implementacao","title":"3. A Solu\u00e7\u00e3o T\u00e9cnica (Implementa\u00e7\u00e3o)","text":"<p>A escolha da ferramenta foi apenas o primeiro passo. A implementa\u00e7\u00e3o pr\u00e1tica em ambiente Windows revelou desafios de configura\u00e7\u00e3o que poderiam prejudicar a experi\u00eancia do desenvolvedor (DX) e dificultar o onboarding de novos membros no futuro.</p>"},{"location":"management/arquitetura/#31-o-desafio-do-ambiente-the-path-saga","title":"3.1 O Desafio do Ambiente (The PATH Saga)","text":"<p>Ao instalar o MkDocs via <code>pip</code> no Windows, o execut\u00e1vel frequentemente n\u00e3o \u00e9 adicionado automaticamente \u00e0s vari\u00e1veis de ambiente (PATH).</p> <ul> <li>O Sintoma: Ao digitar <code>mkdocs serve</code> no terminal, o sistema retornava o erro: \"O termo 'mkdocs' n\u00e3o \u00e9 reconhecido...\".</li> <li>A Solu\u00e7\u00e3o Fr\u00e1gil: Poder\u00edamos editar manualmente as Vari\u00e1veis de Ambiente do Windows.</li> <li>O Risco: Isso cria uma depend\u00eancia da configura\u00e7\u00e3o da m\u00e1quina local (\"Works on my machine\"). Se formatarmos o PC, perdemos a capacidade de rodar a doc.</li> </ul>"},{"location":"management/arquitetura/#32-a-camada-de-abstracao-makefile","title":"3.2 A Camada de Abstra\u00e7\u00e3o (Makefile)","text":"<p>Para resolver a fragilidade do ambiente, decidimos n\u00e3o invocar o bin\u00e1rio do MkDocs diretamente. Em vez disso, utilizamos o <code>Makefile</code> (j\u00e1 presente no projeto devido ao Go) como um \"controle remoto\" ou wrapper.</p> <p>Adotamos a estrat\u00e9gia de invocar o m\u00f3dulo Python diretamente (<code>python -m mkdocs</code>). Isso garante que, se o Python estiver instalado, a documenta\u00e7\u00e3o vai rodar, ignorando completamente se o <code>mkdocs.exe</code> est\u00e1 no PATH ou n\u00e3o.</p>"},{"location":"management/arquitetura/#33-snippet-de-implementacao","title":"3.3 Snippet de Implementa\u00e7\u00e3o","text":"<p>Abaixo, o trecho do <code>Makefile</code> que padroniza os comandos de documenta\u00e7\u00e3o. Note a simplicidade exposta para o usu\u00e1rio final versus a robustez do comando real executado.</p> <pre><code># ==============================================================================\n# DOCUMENTATION (Docs as Code)\n# ==============================================================================\n\n# Instala as depend\u00eancias listadas no requirements.txt (MkDocs + Tema Material + Plugins)\ndocs-install:\n pip install -r docs/requirements.txt\n\n# Roda o servidor local com Hot-Reload (Atualiza ao salvar o arquivo)\n# Uso: python -m mkdocs garante execu\u00e7\u00e3o mesmo sem PATH configurado no Windows\ndocs-serve:\n python -m mkdocs serve\n\n# Compila o site est\u00e1tico para a pasta \"site/\" (Usado antes do deploy)\ndocs-build:\n python -m mkdocs build\n\n# Faz o deploy manual para o GitHub Pages\ndocs-deploy:\n python -m mkdocs gh-deploy --force\n</code></pre> <p>Resultado: O desenvolvedor n\u00e3o precisa saber comandos Python ou configurar PATHs complexos. Ele apenas digita make docs-serve e o ambiente funciona. Reduzimos a carga cognitiva e aumentamos a confiabilidade do processo.</p>"},{"location":"management/arquitetura/#4-diagramas-e-visualizacao-diagrams-as-code","title":"4. Diagramas e Visualiza\u00e7\u00e3o (Diagrams as Code)","text":"<p>Uma documenta\u00e7\u00e3o de engenharia robusta depende de fluxogramas e diagramas de arquitetura claros. No entanto, o m\u00e9todo tradicional (criar imagens em ferramentas externas como Visio ou Lucidchart, exportar como <code>.png</code> e colar no Markdown) gera um D\u00e9bito T\u00e9cnico de Documenta\u00e7\u00e3o imediato:</p> <ul> <li>Imagens bin\u00e1rias n\u00e3o s\u00e3o version\u00e1veis: O Git n\u00e3o consegue mostrar o \"diff\" (o que mudou) dentro de uma imagem PNG.</li> <li>Obsolesc\u00eancia R\u00e1pida: Se a l\u00f3gica do c\u00f3digo muda, o diagrama torna-se \"mentiroso\" pois o esfor\u00e7o para recriar a imagem \u00e9 alto.</li> </ul>"},{"location":"management/arquitetura/#41-a-decisao-mermaidjs","title":"4.1 A Decis\u00e3o: Mermaid.js","text":"<p>Para resolver isso, adotamos o Mermaid.js, uma ferramenta que renderiza gr\u00e1ficos vetoriais diretamente no navegador a partir de defini\u00e7\u00f5es de texto simples. Isso estende a filosofia \"Docs as Code\" para a parte visual.</p> <p>Comparativo de Manuten\u00e7\u00e3o:</p> <ul> <li>Legado (Imagem): Abrir software gr\u00e1fico -&gt; Editar -&gt; Exportar -&gt; Substituir arquivo -&gt; Commit.</li> <li>Atual (Mermaid): Editar texto no Markdown -&gt; Commit.</li> </ul> <p>Exemplo Pr\u00e1tico: Ao escrevermos o seguinte bloco de c\u00f3digo:</p> <p>mermaid graph LR A[Usu\u00e1rio] --&gt;|Upload| B(Go Backend) B --&gt;|Processa| C{Sucesso?} C --&gt;|Sim| D[Retorna JSON] C --&gt;|N\u00e3o| E[Retorna Erro]</p> <pre><code>    graph LR\n      A[Usu\u00e1rio] --&gt;|Upload| B(Go Backend)\n      B --&gt;|Processa| C{Sucesso?}\n      C --&gt;|Sim| D[Retorna JSON]\n      C --&gt;|N\u00e3o| E[Retorna Erro]</code></pre> <p>O MkDocs intercepta esse bloco e desenha o fluxograma automaticamente na tela do usu\u00e1rio final.</p>"},{"location":"management/arquitetura/#42-configuracao-tecnica","title":"4.2 Configura\u00e7\u00e3o T\u00e9cnica","text":"<p>Para que o MkDocs entenda essa sintaxe especial (e n\u00e3o a trate como c\u00f3digo comum), realizamos duas configura\u00e7\u00f5es de infraestrutura:</p> <ol> <li>No Build (<code>mkdocs.yml</code>): Ativa\u00e7\u00e3o da extens\u00e3o <code>pymdownx.superfences</code>. \u00c9 ela que permite injetar scripts (como o do Mermaid) dentro de blocos de c\u00f3digo cercados (fences).</li> <li>No Editor (<code>.vscode/settings.json</code>): Ajuste das configura\u00e7\u00f5es do workspace para que o linter (verificador de sintaxe) do Markdown entenda que <code>mermaid</code> \u00e9 uma linguagem v\u00e1lida, evitando falsos alertas de erro durante a escrita.</li> </ol>"},{"location":"management/arquitetura/#5-estrategia-de-deploy-cicd","title":"5. Estrat\u00e9gia de Deploy (CI/CD)","text":"<p>Para garantir que a documenta\u00e7\u00e3o esteja acess\u00edvel globalmente para stakeholders e n\u00e3o apenas na m\u00e1quina local do desenvolvedor, definimos uma estrat\u00e9gia de publica\u00e7\u00e3o baseada em hospedagem est\u00e1tica.</p>"},{"location":"management/arquitetura/#51-infraestrutura-github-pages","title":"5.1 Infraestrutura: GitHub Pages","text":"<p>Optamos pelo GitHub Pages como plataforma de hospedagem.</p> <ul> <li>Custo Zero: Integrado ao plano gratuito do GitHub.</li> <li>HTTPS Nativo: Seguran\u00e7a autom\u00e1tica sem configura\u00e7\u00e3o de certificados.</li> <li>Separa\u00e7\u00e3o de Responsabilidades (Branching Strategy):</li> <li>Branch <code>main</code>: Cont\u00e9m a Fonte da Verdade (C\u00f3digo Fonte + Markdown Bruto).</li> <li>Branch <code>gh-pages</code>: Cont\u00e9m apenas o Artefato Compilado (HTML/CSS/JS final). Essa branch \u00e9 \u00f3rf\u00e3 e gerenciada automaticamente pelo script de deploy.</li> </ul>"},{"location":"management/arquitetura/#52-mecanismo-de-deploy-automacao","title":"5.2 Mecanismo de Deploy (Automa\u00e7\u00e3o)","text":"<p>Embora o GitHub Actions seja o padr\u00e3o de mercado para CI/CD, para esta fase inicial do projeto, decidimos, arquiteturalmente, n\u00e3o automatizar o deploy a cada commit.</p> <p>Decis\u00e3o: Deploy Manual Controlado (\"Gatekeeper Strategy\") Implementamos o comando <code>make docs-deploy</code> no Makefile.</p> <ul> <li>O Risco da Automa\u00e7\u00e3o Total: Um pipeline de \"Deploy on Push\" publicaria imediatamente qualquer altera\u00e7\u00e3o salva. Isso cria o risco de expor rascunhos incompletos, se\u00e7\u00f5es \"To-Do\" ou documenta\u00e7\u00e3o de funcionalidades que ainda n\u00e3o foram mergeadas no c\u00f3digo principal.</li> <li>A Vantagem do Manual: O deploy manual devolve o controle ao engenheiro. A documenta\u00e7\u00e3o s\u00f3 \u00e9 atualizada em produ\u00e7\u00e3o quando o desenvolvedor explicitamente valida o conte\u00fado localmente (<code>make docs-serve</code>) e decide que a vers\u00e3o est\u00e1 est\u00e1vel para consumo p\u00fablico.</li> </ul> <p>Comando T\u00e9cnico: O Makefile executa: <code>python -m mkdocs gh-deploy --force</code> (Nota: A flag <code>--force</code> \u00e9 utilizada intencionalmente para sobrescrever o hist\u00f3rico da branch <code>gh-pages</code>, mantendo o reposit\u00f3rio leve e focado apenas na vers\u00e3o atual do site).</p>"}]}